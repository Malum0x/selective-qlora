{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "135bb16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub\n",
    "#!pip install trl\n",
    "#!pip install transformers\n",
    "#!pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb07f81",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# if issues with versions:   \n",
    "#!pip uninstall -y transformers peft trl bitsandbytes accelerate wandb\n",
    "#!pip install --upgrade transformers peft trl bitsandbytes accelerate wandb --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8788dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 8667.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/bart/Desktop/selective-qlora/selective-qlora/base_model'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hf model download \n",
    "from huggingface_hub import snapshot_download\n",
    "download = snapshot_download(repo_id=\"Qwen/Qwen2.5-3B-Instruct\", repo_type='model', local_dir='base_model')\n",
    "download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd46039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" #i had wandb issues \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2797dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bits and bytes: \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6daa02af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current model requires 512 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "Loading weights: 100%|██████████| 434/434 [00:40<00:00, 10.62it/s, Materializing param=model.norm.weight]                              \n"
     ]
    }
   ],
   "source": [
    "# load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3b-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2862546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 15787.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/bart/Desktop/selective-qlora/selective-qlora/dataset'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset download \n",
    "from huggingface_hub import snapshot_download\n",
    "download_dataset = snapshot_download(repo_id=\"teknium/OpenHermes-2.5\", repo_type='dataset', local_dir='dataset')\n",
    "download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f802fd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       conversations        source  \\\n",
      "0  [{'from': 'human', 'value': 'Every day, a tree...  airoboros2.2   \n",
      "1  [{'from': 'human', 'value': 'In analytical che...  airoboros2.2   \n",
      "2  [{'from': 'human', 'value': 'A rectangular gar...  airoboros2.2   \n",
      "3  [{'from': 'human', 'value': 'What was the purp...  airoboros2.2   \n",
      "4  [{'from': 'human', 'value': 'A man claims he c...  airoboros2.2   \n",
      "\n",
      "          category  skip_prompt_formatting   id language title model_name  \\\n",
      "0             orca                     0.0  NaN      NaN   NaN        NaN   \n",
      "1  multiple_choice                     0.0  NaN      NaN   NaN        NaN   \n",
      "2             orca                     0.0  NaN      NaN   NaN        NaN   \n",
      "3          general                     0.0  NaN      NaN   NaN        NaN   \n",
      "4             orca                     0.0  NaN      NaN   NaN        NaN   \n",
      "\n",
      "   custom_instruction topic system_prompt model avatarUrl  views hash  idx  \n",
      "0                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n",
      "1                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n",
      "2                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n",
      "3                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n",
      "4                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Current model requires 512 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "Loading weights: 100%|██████████| 434/434 [00:00<00:00, 2334.87it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 30 hardest samples \n"
     ]
    }
   ],
   "source": [
    "# selective dataset technique \n",
    "# forward pass on dataset - without training\n",
    "# save the 30% hardest ones\n",
    "# https://medium.com/@njorogeofrey73/forward-pass-3f716ed71f19 \n",
    "\n",
    "# imports again ( why? )\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# load datasetf\n",
    "dataset_hermes = 'dataset/openhermes2_5.json'\n",
    "df = pd.read_json(dataset_hermes)\n",
    "print(df.head())\n",
    "\n",
    "# Tokenize data\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Every day, a tree drops 7 leaves. How many leaves would it drop in a month of February in a non-leap year? Include your logic\"\".\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Here's the logic behind this:\\n\\n1. We know that February has 28 days in a non-leap year.\\n2. ... (itd.)\"}\n",
    "]\n",
    "\n",
    "#convert data to match model // dataset \n",
    "def convert_to_messages(row_conversations):\n",
    "    messages = []\n",
    "    role_map = {\n",
    "        \"system\": \"system\",\n",
    "        \"human\": \"user\",\n",
    "        \"gpt\": \"assistant\"\n",
    "    }\n",
    "\n",
    "    for msg in row_conversations:\n",
    "        role = role_map.get(msg[\"from\"])\n",
    "        content = msg[\"value\"]\n",
    "\n",
    "        if role == \"system\" and not content.strip():\n",
    "            continue\n",
    "\n",
    "        messages.append({\"role\": role, \"content\": content})\n",
    "    return messages\n",
    "df = df.head(100).copy() # testing on 100 samples (have to add batch_size to increase speed test)\n",
    "df['formatted_messages'] = df['conversations'].apply(convert_to_messages)\n",
    "\n",
    "def calculate_loss(msgs): \n",
    "    text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "    ).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        return outputs.loss.item()\n",
    "    \n",
    "model.eval()\n",
    "df['loss'] = df['formatted_messages'].apply(calculate_loss)\n",
    "\n",
    "df_sorted = df.sort_values(by='loss', ascending=False)\n",
    "top_30_percent = int(len(df_sorted) * 0.3)\n",
    "hard_samples = df_sorted.head(top_30_percent)\n",
    "\n",
    "print(f\"we have {len(hard_samples)} hardest samples \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "261811d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5: \n",
      "\n",
      "         loss  \\\n",
      "57  5.642434   \n",
      "40  5.023724   \n",
      "76  4.731190   \n",
      "77  4.092717   \n",
      "96  3.996943   \n",
      "\n",
      "                                                                                                                                                                                              conversations  \n",
      "57  [{'from': 'system', 'value': 'You are a world class trivia AI - provide accurate, succinct responses.'}, {'from': 'human', 'value': 'Stories passed down through generations, this palindrome often ...  \n",
      "40               [{'from': 'human', 'value': 'Do you know any jokes about animals with ailments?'}, {'from': 'gpt', 'value': 'What's worse than a centipede with sore feet? A giraffe with a sore throat'}]  \n",
      "76                                          [{'from': 'human', 'value': 'Do you know any jokes about librarians?'}, {'from': 'gpt', 'value': 'Why do librarians like the wind? It says, \"Shhh!\" all day!'}]  \n",
      "77  [{'from': 'human', 'value': 'How many apples does Sally have if she started with 8 oranges and ate 3 pears?'}, {'from': 'gpt', 'value': 'There is not enough information to determine how many apple...  \n",
      "96  [{'from': 'system', 'value': 'You are a world class trivia AI - provide accurate, succinct responses.'}, {'from': 'human', 'value': 'About 90% of the population of what Southeast Asian country liv...  \n",
      "worse 5: \n",
      "\n",
      "         loss  \\\n",
      "32  0.461909   \n",
      "24  0.424308   \n",
      "58  0.417521   \n",
      "69  0.412736   \n",
      "99  0.289743   \n",
      "\n",
      "                                                                                                                                                                                              conversations  \n",
      "32  [{'from': 'human', 'value': 'Create a Python script to implement a doubly linked list data structure with methods for adding nodes at the beginning, end, and middle of the list, as well as deletin...  \n",
      "24  [{'from': 'human', 'value': 'Develop a C++ program that simulates a basic blockchain system. The program should include functionality for creating blocks, adding transactions, validating the chain...  \n",
      "58  [{'from': 'human', 'value': 'Create a PHP script that connects to a MySQL database and performs CRUD operations on a \"products\" table. The table should contain columns for product ID, name, descri...  \n",
      "69  [{'from': 'human', 'value': 'Develop a Python FastAPI application with CRUD endpoints for managing a collection of products, including creating, reading, updating, and deleting products. Integrate...  \n",
      "99  [{'from': 'human', 'value': 'Write a C program that calculates the shortest path between two nodes in a weighted graph using Dijkstra's algorithm. The graph should be represented using adjacency m...  \n"
     ]
    }
   ],
   "source": [
    "# check details\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "print(f\"top 5: \\n\\n {df_sorted[['loss', 'conversations']].head(5)}\")\n",
    "print(f\"worse 5: \\n\\n {df_sorted[['loss', 'conversations']].tail(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de921a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:04<00:00, 2224.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "# make it faster, still tests\n",
    "# take dialog, tokenize, run model, eval, save loss for every example \n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(example):\n",
    "    msgs = convert_to_messages(example['conversations'])\n",
    "    text = tokenizer.apply_chat_template(msgs, tokenize=False)\n",
    "    tokenized = tokenizer(text, truncation=True,  max_length=2048)\n",
    "    #tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "full_ds = Dataset.from_pandas(df[['conversations']])\n",
    "full_ds = full_ds.map(preprocess_function, remove_columns=['conversations'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "loader = DataLoader(full_ds, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "all_losses = []\n",
    "model.eval()\n",
    "\n",
    "# reduction='none' -> loss per token\n",
    "loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "print(\"start\")\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs) # forward pass\n",
    "        logits = outputs.logits\n",
    "        labels = inputs[\"input_ids\"]\n",
    "        # calculate loss per token, logits predicting next token\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        # group batch and calculate the average with padding mask\n",
    "        loss = loss.view(shift_labels.size(0), -1)\n",
    "        mask = (shift_labels != tokenizer.pad_token_id).float()\n",
    "        # calculate the average only for tokens (not for paddings)\n",
    "        actual_loss = (loss * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        all_losses.extend(actual_loss.cpu().tolist())\n",
    "\n",
    "\n",
    "df['loss'] = all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aabee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m df = df.head(\u001b[32m10000\u001b[39m).copy()\n\u001b[32m      4\u001b[39m full_ds = Dataset.from_pandas(df[[\u001b[33m'\u001b[39m\u001b[33mconversations\u001b[39m\u001b[33m'\u001b[39m]])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m full_ds = full_ds.map(\u001b[43mpreprocess_function\u001b[49m, remove_columns=[\u001b[33m'\u001b[39m\u001b[33mconversations\u001b[39m\u001b[33m'\u001b[39m], num_proc=\u001b[32m4\u001b[39m)\n\u001b[32m      6\u001b[39m loader = DataLoader(full_ds, batch_size=\u001b[32m8\u001b[39m, collate_fn=data_collator)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnew size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, test\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[31mNameError\u001b[39m: name 'preprocess_function' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(dataset_hermes)\n",
    "df = df.head(10000).copy()\n",
    "\n",
    "full_ds = Dataset.from_pandas(df[['conversations']])\n",
    "full_ds = full_ds.map(preprocess_function, remove_columns=['conversations'], num_proc=4)\n",
    "loader = DataLoader(full_ds, batch_size=8, collate_fn=data_collator)\n",
    "print(f\"new size {len(df)}, test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a11e37",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_839598/408678251.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#show\u001b[39;00m\n\u001b[32m      2\u001b[39m total_count = len(df)\n\u001b[32m      3\u001b[39m top_30_count = int(total_count * \u001b[32m0.3\u001b[39m)\n\u001b[32m      4\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_sorted = df.sort_values(by=\u001b[33m'loss'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m print(f\"Total samples {total_count}\")\n\u001b[32m      8\u001b[39m print(f\"top 30% (hardest ones) {top_30_count}\")\n",
      "\u001b[32m~/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7185\u001b[39m             )\n\u001b[32m   7186\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7187\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7188\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7189\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7190\u001b[39m \n\u001b[32m   7191\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7192\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'loss'"
     ]
    }
   ],
   "source": [
    "#show \n",
    "total_count = len(df)\n",
    "top_30_count = int(total_count * 0.3)\n",
    "\n",
    "df_sorted = df.sort_values(by='loss', ascending=False)\n",
    "\n",
    "print(f\"Total samples {total_count}\")\n",
    "print(f\"top 30% (hardest ones) {top_30_count}\")\n",
    "print(f\"The average loss {df['loss'].mean():.4f}\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "print(df_sorted[['loss', 'conversations']].head(5))\n",
    "print(df_sorted[['loss', 'conversations']].tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final \n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def filter_hard_examples(mode, loader, tokenzier, df, output_filename='filtered_dataset.json'):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "\n",
    "    # calculate loss (inference)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none', df, output_filename=\"filtered_dataset\")\n",
    "    print(f\"Starting loss calculation for {len(df)} samples\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            # transfer to gpu\n",
    "            inputs = {k: v.to(model.device)  for k, v in batch.items()}\n",
    "            labels = inputs[\"input_ids\"]\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # shift to Causal language modeling\n",
    "            shift_logits = lotis[..., :-1, :].contiguous()\n",
    "            shift_labels = loss.view(shift_labes.size(0), -1)\n",
    "\n",
    "            # mask for padding \n",
    "            mask = (shift_labels != tokenizer.pad_token_id).float()\n",
    "\n",
    "            # average loss per sample\n",
    "            actual_loss = (loss * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "            all_losses.extend(actual_loss.cpu().tolist())\n",
    "            \n",
    "    # sort and choosing top 30%\n",
    "    df['loss'] = all_losses\n",
    "    num_to_keep = int(len(df) * top_k_percent)\n",
    "\n",
    "    # descending sort (highest loss at the top) \n",
    "    df_sorted = df.sort_values(by='loss', ascending=False)\n",
    "    df_filtered = df_sorted.head(num_to_keep)\n",
    "\n",
    "    print(f\"Filtering complete. Keeping {len(df_filtered)} samples with highest loss\")\n",
    "    print(f\"Loss range in new dataset: {df_filtered['loss'].max():.4f} to {df.filtered}\")\n",
    "\n",
    "    # save to json\n",
    "    # delete loss to have clean dataset: \n",
    "\n",
    "    final_output = df_filtered.drop(columns=['loss'])\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        for _, row in final_output.iterrows():\n",
    "            f.write(json.dumps(row.to_dict(), ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"saved filtered dataset to: {output_filename}\")\n",
    "    return df_filtered\n",
    "\n",
    "# run:\n",
    "# filtered_df = filter_hard_samples(model, loader, tokenizer, df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7fa96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
