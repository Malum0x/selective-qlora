{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "135bb16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub\n",
    "#!pip install trl\n",
    "#!pip install transformers\n",
    "#!pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bb07f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if issues with versions:   \n",
    "#!pip uninstall -y transformers peft trl bitsandbytes accelerate wandb\n",
    "#!pip install --upgrade transformers peft trl bitsandbytes accelerate wandb --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8788dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|██████████| 12/12 [01:30<00:00,  7.58s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/bart/Desktop/selective-qlora/selective-qlora/base_model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hf model download \n",
    "from huggingface_hub import snapshot_download\n",
    "download = snapshot_download(repo_id=\"Qwen/Qwen2.5-3B-Instruct\", repo_type='model', local_dir='base_model')\n",
    "download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd46039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" #i had wandb issues \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2797dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bits and bytes: \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6daa02af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current model requires 512 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "Loading weights: 100%|██████████| 434/434 [00:39<00:00, 10.99it/s, Materializing param=model.norm.weight]                              \n"
     ]
    }
   ],
   "source": [
    "# load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3b-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2862546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 6516.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/bart/Desktop/selective-qlora/selective-qlora/dataset'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset download \n",
    "from huggingface_hub import snapshot_download\n",
    "download_dataset = snapshot_download(repo_id=\"teknium/OpenHermes-2.5\", repo_type='dataset', local_dir='dataset')\n",
    "download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f802fd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       conversations        source  \\\n",
      "0  [{'from': 'human', 'value': 'Every day, a tree...  airoboros2.2   \n",
      "1  [{'from': 'human', 'value': 'In analytical che...  airoboros2.2   \n",
      "2  [{'from': 'human', 'value': 'A rectangular gar...  airoboros2.2   \n",
      "3  [{'from': 'human', 'value': 'What was the purp...  airoboros2.2   \n",
      "4  [{'from': 'human', 'value': 'A man claims he c...  airoboros2.2   \n",
      "\n",
      "          category  skip_prompt_formatting   id language title model_name  \\\n",
      "0             orca                     0.0  NaN      NaN   NaN        NaN   \n",
      "1  multiple_choice                     0.0  NaN      NaN   NaN        NaN   \n",
      "2             orca                     0.0  NaN      NaN   NaN        NaN   \n",
      "3          general                     0.0  NaN      NaN   NaN        NaN   \n",
      "4             orca                     0.0  NaN      NaN   NaN        NaN   \n",
      "\n",
      "   custom_instruction topic system_prompt model avatarUrl  views hash  idx  \n",
      "0                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n",
      "1                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n",
      "2                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n",
      "3                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n",
      "4                 NaN   NaN           NaN   NaN       NaN    NaN  NaN  NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Current model requires 512 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "Loading weights: 100%|██████████| 434/434 [00:00<00:00, 2247.07it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 30 hardest samples \n"
     ]
    }
   ],
   "source": [
    "# selective dataset technique \n",
    "# forward pass on dataset - without training\n",
    "# save the 30% hardest ones\n",
    "# https://medium.com/@njorogeofrey73/forward-pass-3f716ed71f19 \n",
    "\n",
    "# imports again ( why? )\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# load datasetf\n",
    "dataset_hermes = 'dataset/openhermes2_5.json'\n",
    "df = pd.read_json(dataset_hermes)\n",
    "print(df.head())\n",
    "\n",
    "# Tokenize data\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Every day, a tree drops 7 leaves. How many leaves would it drop in a month of February in a non-leap year? Include your logic\"\".\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Here's the logic behind this:\\n\\n1. We know that February has 28 days in a non-leap year.\\n2. ... (itd.)\"}\n",
    "]\n",
    "\n",
    "#convert data to match model // dataset \n",
    "def convert_to_messages(row_conversations):\n",
    "    messages = []\n",
    "    role_map = {\n",
    "        \"system\": \"system\",\n",
    "        \"human\": \"user\",\n",
    "        \"gpt\": \"assistant\"\n",
    "    }\n",
    "\n",
    "    for msg in row_conversations:\n",
    "        role = role_map.get(msg[\"from\"])\n",
    "        content = msg[\"value\"]\n",
    "\n",
    "        if role == \"system\" and not content.strip():\n",
    "            continue\n",
    "\n",
    "        messages.append({\"role\": role, \"content\": content})\n",
    "    return messages\n",
    "df = df.head(100).copy() # testing on 100 samples (have to add batch_size to increase speed test)\n",
    "df['formatted_messages'] = df['conversations'].apply(convert_to_messages)\n",
    "\n",
    "def calculate_loss(msgs): \n",
    "    text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "    ).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        return outputs.loss.item()\n",
    "    \n",
    "model.eval()\n",
    "df['loss'] = df['formatted_messages'].apply(calculate_loss)\n",
    "\n",
    "df_sorted = df.sort_values(by='loss', ascending=False)\n",
    "top_30_percent = int(len(df_sorted) * 0.3)\n",
    "hard_samples = df_sorted.head(top_30_percent)\n",
    "\n",
    "print(f\"we have {len(hard_samples)} hardest samples \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "261811d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5: \n",
      "\n",
      "         loss  \\\n",
      "57  5.642434   \n",
      "40  5.023724   \n",
      "76  4.731190   \n",
      "77  4.092717   \n",
      "96  3.996943   \n",
      "\n",
      "                                                                                                                                                                                              conversations  \n",
      "57  [{'from': 'system', 'value': 'You are a world class trivia AI - provide accurate, succinct responses.'}, {'from': 'human', 'value': 'Stories passed down through generations, this palindrome often ...  \n",
      "40               [{'from': 'human', 'value': 'Do you know any jokes about animals with ailments?'}, {'from': 'gpt', 'value': 'What's worse than a centipede with sore feet? A giraffe with a sore throat'}]  \n",
      "76                                          [{'from': 'human', 'value': 'Do you know any jokes about librarians?'}, {'from': 'gpt', 'value': 'Why do librarians like the wind? It says, \"Shhh!\" all day!'}]  \n",
      "77  [{'from': 'human', 'value': 'How many apples does Sally have if she started with 8 oranges and ate 3 pears?'}, {'from': 'gpt', 'value': 'There is not enough information to determine how many apple...  \n",
      "96  [{'from': 'system', 'value': 'You are a world class trivia AI - provide accurate, succinct responses.'}, {'from': 'human', 'value': 'About 90% of the population of what Southeast Asian country liv...  \n",
      "worse 5: \n",
      "\n",
      "         loss  \\\n",
      "32  0.461909   \n",
      "24  0.424308   \n",
      "58  0.417521   \n",
      "69  0.412736   \n",
      "99  0.289743   \n",
      "\n",
      "                                                                                                                                                                                              conversations  \n",
      "32  [{'from': 'human', 'value': 'Create a Python script to implement a doubly linked list data structure with methods for adding nodes at the beginning, end, and middle of the list, as well as deletin...  \n",
      "24  [{'from': 'human', 'value': 'Develop a C++ program that simulates a basic blockchain system. The program should include functionality for creating blocks, adding transactions, validating the chain...  \n",
      "58  [{'from': 'human', 'value': 'Create a PHP script that connects to a MySQL database and performs CRUD operations on a \"products\" table. The table should contain columns for product ID, name, descri...  \n",
      "69  [{'from': 'human', 'value': 'Develop a Python FastAPI application with CRUD endpoints for managing a collection of products, including creating, reading, updating, and deleting products. Integrate...  \n",
      "99  [{'from': 'human', 'value': 'Write a C program that calculates the shortest path between two nodes in a weighted graph using Dijkstra's algorithm. The graph should be represented using adjacency m...  \n"
     ]
    }
   ],
   "source": [
    "# check details\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "print(f\"top 5: \\n\\n {df_sorted[['loss', 'conversations']].head(5)}\")\n",
    "print(f\"worse 5: \\n\\n {df_sorted[['loss', 'conversations']].tail(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34de921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 10000/10000 [00:02<00:00, 4712.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new size 10000, test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(dataset_hermes)\n",
    "df = df.head(10000).copy()\n",
    "\n",
    "full_ds = Dataset.from_pandas(df[['conversations']])\n",
    "full_ds = full_ds.map(preprocess_function, remove_columns=['conversations'], num_proc=4)\n",
    "loader = DataLoader(full_ds, batch_size=8, collate_fn=data_collator)\n",
    "print(f\"new size {len(df)}, test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0291747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2160.99 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "# make it faster\n",
    "# take dialog, tokenize, run model, eval, save loss for every example \n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(example):\n",
    "    msgs = convert_to_messages(example['conversations'])\n",
    "    text = tokenizer.apply_chat_template(msgs, tokenize=False)\n",
    "    tokenized = tokenizer(text, truncation=True,  max_length=2048)\n",
    "    #tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "full_ds = Dataset.from_pandas(df[['conversations']])\n",
    "full_ds = full_ds.map(preprocess_function, remove_columns=['conversations'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "loader = DataLoader(full_ds, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "all_losses = []\n",
    "model.eval()\n",
    "\n",
    "# reduction='none' -> loss per token\n",
    "loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "print(\"start\")\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs) # forward pass\n",
    "        logits = outputs.logits\n",
    "        labels = inputs[\"input_ids\"]\n",
    "        # calculate loss per token, logits predicting next token\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        # group batch and calculate the average with padding mask\n",
    "        loss = loss.view(shift_labels.size(0), -1)\n",
    "        mask = (shift_labels != tokenizer.pad_token_id).float()\n",
    "        # calculate the average only for tokens (not for paddings)\n",
    "        actual_loss = (loss * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        all_losses.extend(actual_loss.cpu().tolist())\n",
    "\n",
    "\n",
    "df['loss'] = all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aabee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 100/100 [00:00<00:00, 175.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new size 100, test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73a11e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples 100\n",
      "top 30% (hardest ones) 30\n",
      "The average loss 1.6759\n",
      "        loss  \\\n",
      "57  5.543040   \n",
      "76  4.692902   \n",
      "40  4.562225   \n",
      "96  3.796139   \n",
      "77  3.568712   \n",
      "\n",
      "                                                                                                                                                                                                                                                                    conversations  \n",
      "57          [{'from': 'system', 'value': 'You are a world class trivia AI - provide accurate, succinct responses.'}, {'from': 'human', 'value': 'Stories passed down through generations, this palindrome often involves legends and myths.'}, {'from': 'gpt', 'value': 'sagas'}]  \n",
      "76                                                                                                                [{'from': 'human', 'value': 'Do you know any jokes about librarians?'}, {'from': 'gpt', 'value': 'Why do librarians like the wind? It says, \"Shhh!\" all day!'}]  \n",
      "40                                                                                     [{'from': 'human', 'value': 'Do you know any jokes about animals with ailments?'}, {'from': 'gpt', 'value': 'What's worse than a centipede with sore feet? A giraffe with a sore throat'}]  \n",
      "96  [{'from': 'system', 'value': 'You are a world class trivia AI - provide accurate, succinct responses.'}, {'from': 'human', 'value': 'About 90% of the population of what Southeast Asian country lives on its largest island, Java?'}, {'from': 'gpt', 'value': 'Indonesia'}]  \n",
      "77                                                            [{'from': 'human', 'value': 'How many apples does Sally have if she started with 8 oranges and ate 3 pears?'}, {'from': 'gpt', 'value': 'There is not enough information to determine how many apples Sally has.'}]  \n",
      "        loss  \\\n",
      "32  0.428893   \n",
      "24  0.410494   \n",
      "58  0.401400   \n",
      "69  0.390046   \n",
      "99  0.274459   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                  conversations  \n",
      "32  [{'from': 'human', 'value': 'Create a Python script to implement a doubly linked list data structure with methods for adding nodes at the beginning, end, and middle of the list, as well as deleting nodes by value.'}, {'from': 'gpt', 'value': '```python\n",
      "class Node:\n",
      "    def __init__(self, data):\n",
      " ...  \n",
      "24  [{'from': 'human', 'value': 'Develop a C++ program that simulates a basic blockchain system. The program should include functionality for creating blocks, adding transactions, validating the chain, and mining new blocks.\n",
      "PLAINFORMAT'}, {'from': 'gpt', 'value': '#include <iostream>\n",
      "#include <ctim...  \n",
      "58  [{'from': 'human', 'value': 'Create a PHP script that connects to a MySQL database and performs CRUD operations on a \"products\" table. The table should contain columns for product ID, name, description, price, and quantity. The script should support command line arguments for specifying the oper...  \n",
      "69  [{'from': 'human', 'value': 'Develop a Python FastAPI application with CRUD endpoints for managing a collection of products, including creating, reading, updating, and deleting products. Integrate Swagger/OpenAPI documentation for the API.'}, {'from': 'gpt', 'value': 'To create a FastAPI applica...  \n",
      "99  [{'from': 'human', 'value': 'Write a C program that calculates the shortest path between two nodes in a weighted graph using Dijkstra's algorithm. The graph should be represented using adjacency matrices.\n",
      "PLAINFORMAT'}, {'from': 'gpt', 'value': '#include <stdio.h>\n",
      "#include <limits.h>\n",
      "\n",
      "#define V ...  \n"
     ]
    }
   ],
   "source": [
    "total_count = len(df)\n",
    "top_30_count = int(total_count * 0.3)\n",
    "\n",
    "df_sorted = df.sort_values(by='loss', ascending=False)\n",
    "\n",
    "print(f\"Total samples {total_count}\")\n",
    "print(f\"top 30% (hardest ones) {top_30_count}\")\n",
    "print(f\"The average loss {df['loss'].mean():.4f}\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "print(df_sorted[['loss', 'conversations']].head(5))\n",
    "print(df_sorted[['loss', 'conversations']].tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff9fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
